{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to run the program:\n",
    "#### 1- Go to cell named main program\n",
    "#### 2- Set the variable 'N' which is the dimension of NxN maze\n",
    "#### 3- Run all the file and see the results details at results cells at the end of the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_maze(N):\n",
    "    \"\"\"\n",
    "    Given a dimension N the functions builds random N*N Maze\n",
    "    \n",
    "    Args:        \n",
    "        N: Dimension of the maze\n",
    "    \n",
    "    Returns:\n",
    "       maze: [N, N] shaped matrix representing the maze.\n",
    "    \"\"\"\n",
    "    randRow = N\n",
    "    randCol = N\n",
    "    steps = N\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    mazeMap = [[0 for x in range(randCol)] for y in range(randRow)]\n",
    "    mazeMap[i][j] = 'S'\n",
    "\n",
    "    while steps != 0:\n",
    "        iORj = random.choice([True, False])\n",
    "        incORdec = random.randint(0, 5)\n",
    "        if iORj and (incORdec > 0) and i != randRow - 1 and mazeMap[i + 1][j] != 'S' and mazeMap[i + 1][j] != '.':\n",
    "            i = i + 1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        elif iORj == False and (incORdec > 0) and j != randCol-1 and mazeMap[i][j+1] != 'S' and mazeMap[i][j + 1] != '.':\n",
    "            j = j + 1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        elif iORj and (incORdec == 0) and i != 0 and mazeMap[i-1][j] != 'S' and mazeMap[i-1][j] != '.':\n",
    "            i = i - 1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        elif iORj == False and (incORdec == 0) and j != 0 and mazeMap[i][j-1] != 'S' and mazeMap[i][j-1] != '.':\n",
    "            j = j -1\n",
    "            mazeMap[i][j] = '.'\n",
    "            if steps == 1:\n",
    "                mazeMap[i][j] = 'E'\n",
    "        else:\n",
    "            continue\n",
    "        steps = steps - 1\n",
    "\n",
    "    ii = 0\n",
    "    jj = 0\n",
    "\n",
    "    for ii in range(0, randRow):\n",
    "        for jj in range(0, randCol):\n",
    "            iORj = random.choice([True, False])\n",
    "            if mazeMap[ii][jj] != 'S':\n",
    "                if mazeMap[ii][jj] != '.':\n",
    "                    if mazeMap[ii][jj] != 'E':\n",
    "                        if iORj:\n",
    "                            mazeMap[ii][jj] = '#'\n",
    "                        else:\n",
    "                            mazeMap[ii][jj] = '.'\n",
    "    return mazeMap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_indx(maze,N):\n",
    "    \"\"\"\n",
    "    Finds the index of the goal state 'E'\n",
    "    \n",
    "    Args:\n",
    "        maze: [N, N] shaped matrix representing the maze.\n",
    "        N: Dimension of the maze\n",
    "    \n",
    "    Returns:\n",
    "       Goal state 'E' index\n",
    "    \"\"\"\n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if maze[i][j] == 'E':\n",
    "                return s\n",
    "            s+=1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(maze,N):\n",
    "    \"\"\"\n",
    "    Given a maze compute the next state for each cell\n",
    "    \n",
    "    Args:\n",
    "        maze: [N, N] shaped matrix representing the maze.\n",
    "        N: Dimension of the maze\n",
    "    \n",
    "    Returns:\n",
    "       [N*N, 4] representing the next state of each maze cell.\n",
    "    \"\"\"\n",
    "    indx = np.zeros([N,N])    \n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            indx[i][j]=s\n",
    "            s+=1    \n",
    "    next_state = np.zeros([N*N, 4])\n",
    "    s=0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if(i-1<0 or maze[i-1][j] == '#'):\n",
    "                next_state[s][0] = s\n",
    "            else:\n",
    "                next_state[s][0] = indx[i-1][j]\n",
    "                \n",
    "            if(j+1>N-1 or maze[i][j+1] == '#'):\n",
    "                next_state[s][1] = s\n",
    "            else:\n",
    "                next_state[s][1] = indx[i][j+1]\n",
    "                \n",
    "            if(i+1>N-1 or maze[i+1][j] == '#'):\n",
    "                next_state[s][2] = s\n",
    "            else:\n",
    "                next_state[s][2] = indx[i+1][j]\n",
    "                \n",
    "            if(j-1<0 or maze[i][j-1] == '#'):\n",
    "                next_state[s][3] = s\n",
    "            else:\n",
    "                next_state[s][3] = indx[i][j-1]\n",
    "            \n",
    "            if(maze[i][j] == '#' or ((i-1<0 or maze[i-1][j] == '#') and (j+1>N-1 or maze[i][j+1] == '#') and (i+1>N-1 or maze[i+1][j] == '#') and (j-1<0 or maze[i][j-1] == '#'))):\n",
    "                next_state[s][0] = -1\n",
    "                next_state[s][1] = -1\n",
    "                next_state[s][2] = -1\n",
    "                next_state[s][3] = -1\n",
    "                \n",
    "            if(maze[i][j] == 'E'):\n",
    "                next_state[s][0] = s\n",
    "                next_state[s][1] = s\n",
    "                next_state[s][2] = s\n",
    "                next_state[s][3] = s\n",
    "            s+=1\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, reward, next_state,V_old, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function    \n",
    "    V_new = np.zeros(16)    \n",
    "    # For each state, perform a \"full backup\"\n",
    "    for s in range(16):\n",
    "        v = 0.0\n",
    "        # Look at the possible next actions\n",
    "        for a, action_prob in list(enumerate(policy[s])):           \n",
    "            # For each action, look at the possible next states...\n",
    "            # Calculate the expected value\n",
    "            nxt = next_state[s][a]\n",
    "            # if not a barrier\n",
    "            if(nxt != -1):\n",
    "                v += action_prob * (reward + discount_factor * V_old[int(nxt)])            \n",
    "        V_new[s] = v      \n",
    "    \n",
    "    return np.array(V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(a):\n",
    "    \"\"\"\n",
    "    Helper function to return the index of the best action according to action values. If there is more than 2 actions that have the same value then there is no best action\n",
    "    Args:\n",
    "        a: actions values  \n",
    "        \n",
    "    Returns:\n",
    "        Best action index and returns -1 if there is a tie of 3 actions or more\n",
    "    \"\"\"\n",
    "    if np.array_equal(a,[0,0,0,0]) or np.array_equal(a,[1,0,0,0])or np.array_equal(a,[0,1,0,0]) or np.array_equal(a,[0,0,1,0]) or np.array_equal(a,[0,0,0,1]):\n",
    "        return np.argmax(a)\n",
    "    freq = np.zeros(4)\n",
    "    i=0\n",
    "    while i<4: \n",
    "        j=0\n",
    "        while j<4:\n",
    "            if abs(a[i]-a[j]) < 0.00001:\n",
    "                freq[i] += 1\n",
    "            j+=1\n",
    "        i+=1\n",
    "    max_indx = np.argmax(a)\n",
    "    if freq[max_indx]>2:\n",
    "        return -1\n",
    "    return np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_deterministic(policy):\n",
    "    \"\"\"\n",
    "    Given a policy the function checks wether it is detereministic policy or not\n",
    "    \n",
    "    Args:\n",
    "        policy: matrix representing the policy.\n",
    "            \n",
    "    Returns:\n",
    "       True if deterministic False otherwise\n",
    "    \"\"\"\n",
    "    rows = policy.shape[0]\n",
    "    cols = policy.shape[1]\n",
    "    for x in range(0, rows):\n",
    "        for y in range(0, cols):\n",
    "            if abs(policy[x,y]-0.25)<0.0001:\n",
    "                return False\n",
    "    return True \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(reward, next_state, goal_indx, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(4)\n",
    "        i=0\n",
    "        for a in range(4):\n",
    "            nxt = next_state[state][a]\n",
    "            if(nxt != -1):\n",
    "                A[i] += (reward + discount_factor * V[int(nxt)])\n",
    "            i = i+1\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([16, 4]) / 4    \n",
    "    policy[goal_indx] = np.zeros(4)\n",
    "    initial_policy = policy.copy()\n",
    "    \n",
    "    V_old = np.zeros(16)\n",
    "    V_new = np.zeros(16)\n",
    "    \n",
    "    k=0\n",
    "    while True:  \n",
    "        print (\"Iteration \",k,\":\")\n",
    "        policy[goal_indx] = np.zeros(4)\n",
    "        policy_old = policy.copy()\n",
    "        # Evaluate the current policy       \n",
    "        V_new = policy_eval_fn(initial_policy, reward, next_state, V_old)        \n",
    "        V_old = V_new.copy()\n",
    "        print(\"Cuurent Values:\")\n",
    "        print(V_new)\n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(16):\n",
    "            # The best action we would take under the currect policy\n",
    "            #chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V_new)\n",
    "            #best_a = np.argmax(action_values)\n",
    "            best_a = best_action(action_values)\n",
    "            \n",
    "            # Greedily update the policy            \n",
    "            #if chosen_a != best_a: \n",
    "            if(best_a == -1):\n",
    "                policy_stable = False\n",
    "            if(best_a != -1):\n",
    "                policy[s] = np.eye(4)[best_a]\n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        \n",
    "        k+=1\n",
    "        print(\"Current Policy Probability distribution: \")\n",
    "        print(policy)\n",
    "        if np.array_equal(policy,policy_old) and k>1 and is_deterministic(policy):\n",
    "            return (policy, V_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(nS,goal_indx,discount_factor = 1.0,theta = 0.0001):\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(4)\n",
    "        i=0\n",
    "        for a in range(4):\n",
    "            nxt = next_state[s][a]\n",
    "            if(nxt != -1):\n",
    "                A[i] += (reward + discount_factor * V[int(nxt)])\n",
    "            i = i+1\n",
    "        return A\n",
    "    \n",
    "    V_old = np.zeros(nS)\n",
    "    V_new = np.zeros(nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        V_old = V_new.copy()\n",
    "        # Update each state...\n",
    "        for s in range(nS):\n",
    "            if s == goal_index:\n",
    "                continue\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V_old)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V_old[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V_new[s] = best_action_value    \n",
    "        print(V_new)\n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    npolicy = np.zeros([nS, 4])\n",
    "    for s in range(nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V_new)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        npolicy[s, best_action] = 1.0\n",
    "    \n",
    "    return npolicy, V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_policy(p):\n",
    "    finished = False\n",
    "    path = []\n",
    "    actions = []\n",
    "    next_square = 0\n",
    "    current_square = 0\n",
    "    while finished == False:\n",
    "        finished = True\n",
    "        for i in range(4):\n",
    "            if p[next_square][i] == 1:\n",
    "                finished = False\n",
    "                if i == 0:\n",
    "                    next_square -= N\n",
    "                    actions.append(\"up\")\n",
    "                elif i == 1:\n",
    "                    next_square += 1 \n",
    "                    actions.append(\"right\")\n",
    "                elif i == 2:\n",
    "                    next_square += N \n",
    "                    actions.append(\"bottom\")\n",
    "                else:\n",
    "                    next_square -= 1\n",
    "                    actions.append(\"left\")\n",
    "                path.append(next_square)\n",
    "    return (path,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxValueState(next_states,V):\n",
    "    max = -10000000000000\n",
    "    index = 0\n",
    "    for ii in range(4):\n",
    "        c_val = V[int(next_states[ii])]\n",
    "        if c_val > max:\n",
    "            max = c_val\n",
    "            index = ii\n",
    "    return ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_value(V,next_state):\n",
    "    finished = False\n",
    "    path = []\n",
    "    actions = []\n",
    "    next_square = 0\n",
    "    current_square = 0\n",
    "    while finished == False:\n",
    "        finished = True\n",
    "        for i in range(4):\n",
    "            new_next_square = getMaxValueState(next_state[next_square],V)\n",
    "            if new_next_square != next_square:\n",
    "                finished = False\n",
    "                next_square = new_next_square\n",
    "                path.append(next_square)\n",
    "                if i == 0:\n",
    "                    actions.append(\"up\")\n",
    "                elif i == 1:\n",
    "                    actions.append(\"right\")\n",
    "                elif i == 2:\n",
    "                    actions.append(\"bottom\")\n",
    "                else:\n",
    "                    actions.append(\"left\")\n",
    "    return (path,actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program \n",
    "#### Put 'N' with any value to generate random NxN maze to be solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 4\n",
    "maze = gen_maze(N)\n",
    "reward = -1\n",
    "discount_factor = 1.0\n",
    "goal_index = get_goal_indx(maze,N) \n",
    "next_state = list(get_next_state(maze,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze Shape\n",
    "#### Assumptions:\n",
    "#### 1- 'S' -> Start State\n",
    "#### 2- '.' -> Normal State\n",
    "#### 3- '#' -> Barrier\n",
    "#### 4- 'E' -> End State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', '.', '.', '#']\n",
      "['#', '.', '#', '#']\n",
      "['.', '.', 'E', '#']\n",
      "['.', '#', '.', '#']\n"
     ]
    }
   ],
   "source": [
    "for ii in range(N):\n",
    "    print(maze[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Maze using Policy Iteration Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :\n",
      "Cuurent Values:\n",
      "[-1. -1. -1.  0.  0. -1.  0.  0. -1. -1.  0.  0. -1.  0. -1.  0.]\n",
      "Current Policy Probability distribution: \n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n",
      "Iteration  1 :\n",
      "Cuurent Values:\n",
      "[-2.   -2.   -2.    0.    0.   -2.    0.    0.   -2.   -1.75  0.    0.\n",
      " -2.    0.   -1.75  0.  ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n",
      "Iteration  2 :\n",
      "Cuurent Values:\n",
      "[-3.     -3.     -3.      0.      0.     -2.9375  0.      0.     -2.9375\n",
      " -2.4375  0.      0.     -3.      0.     -2.3125  0.    ]\n",
      "Current Policy Probability distribution: \n",
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]]\n",
      "Iteration  3 :\n",
      "Cuurent Values:\n",
      "[-4.       -3.984375 -4.        0.        0.       -3.828125  0.\n",
      "  0.       -3.828125 -3.078125  0.        0.       -3.984375  0.\n",
      " -2.734375  0.      ]\n",
      "Current Policy Probability distribution: \n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Iteration  4 :\n",
      "Cuurent Values:\n",
      "[-4.99609375 -4.953125   -4.99609375  0.          0.         -4.6796875\n",
      "  0.          0.         -4.6796875  -3.68359375  0.          0.\n",
      " -4.9453125   0.         -3.05078125  0.        ]\n",
      "Current Policy Probability distribution: \n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "-------------Final Results---------\n",
      "\n",
      "-\n",
      "Policy Probability Distribution:\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Value Function:\n",
      "[-4.99609375 -4.953125   -4.99609375  0.          0.         -4.6796875\n",
      "  0.          0.         -4.6796875  -3.68359375  0.          0.\n",
      " -4.9453125   0.         -3.05078125  0.        ]\n",
      "\n",
      "Path: \n",
      "[1, 5, 9, 10]\n",
      "Actions: \n",
      "['right', 'bottom', 'bottom', 'right']\n",
      "--- Running Time : 0.019726276397705078 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "policy, v = policy_improvement(reward, next_state , goal_index)\n",
    "exec_time = (time.time() - start_time)\n",
    "print(\"\\n\\n-------------Final Results---------\\n\\n-\")\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "path,actions = get_path_policy(policy)\n",
    "print(\"Path: \")\n",
    "print(path)\n",
    "print(\"Actions: \")\n",
    "print(actions)\n",
    "print(\"--- Running Time : %s seconds ---\" % exec_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Maze using Value Iteration Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1.  0.  0.  0. -1.  0.  0.  0. -1.  0.  0.  0.  0.  0.]\n",
      "[-2. -2. -2.  0.  0.  0. -1.  0.  0.  0. -2.  0.  0.  0.  0.  0.]\n",
      "[-3. -3. -2.  0.  0.  0. -1.  0.  0.  0. -2.  0.  0.  0.  0.  0.]\n",
      "[-4. -3. -2.  0.  0.  0. -1.  0.  0.  0. -2.  0.  0.  0.  0.  0.]\n",
      "[-4. -3. -2.  0.  0.  0. -1.  0.  0.  0. -2.  0.  0.  0.  0.  0.]\n",
      "\n",
      "\n",
      "-------------Final Results---------\n",
      "\n",
      "-\n",
      "Value Function:\n",
      "[[-4. -3. -2.  0.]\n",
      " [ 0.  0. -1.  0.]\n",
      " [ 0.  0. -2.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "Path: \n",
      "[1, 2, 6, 7]\n",
      "Actions: \n",
      "['right', 'right', 'bottom', 'right']\n",
      "--- Running Time : 0.009593725204467773 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mpolicy, v = value_iteration(N*N,goal_index)\n",
    "exec_time = (time.time() - start_time)\n",
    "print(\"\\n\\n-------------Final Results---------\\n\\n-\")\n",
    "print(\"Value Function:\")\n",
    "print(v.reshape((N,N)))\n",
    "print(\"\")\n",
    "path = []\n",
    "actions = []\n",
    "path,actions = get_path_policy(policy)\n",
    "print(\"Path: \")\n",
    "print(path)\n",
    "print(\"Actions: \")\n",
    "print(actions)\n",
    "print(\"--- Running Time : %s seconds ---\" % exec_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
